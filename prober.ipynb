{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afsdfasdf', 'asdfasdf', 'asdfasdf,asdfasdfadsf;', 'dfasdfadf']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "x = \"afsdfasdf.asdfasdf.asdfasdf,asdfasdfadsf;?dfasdfadf\"\n",
    "re.split(r'\\.|\\?', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "prompts = json.load(open('prompts.json'))\n",
    "\n",
    "configuration = {\n",
    "    \"Remember\": 10,\n",
    "    \"Understand\": 10,\n",
    "    \"Apply\": 10,\n",
    "    \"Analyze\": 10,\n",
    "    \"Evaluate\": 10,\n",
    "    \"Create\": 10\n",
    "}\n",
    "\n",
    "while False:\n",
    "    input()\n",
    "\n",
    "    sample_category = random.choices(list(configuration.keys()), weights = list(configuration.values()))[0]\n",
    "    sample_prompt = random.choice(prompts[sample_category])\n",
    "\n",
    "    print('\\n' + sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = r'''\\documentclass[12pt]{article}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage{setspace}\n",
    "\\usepackage{listings}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{fullpage}\n",
    "\n",
    "\\onehalfspacing\n",
    "\n",
    "\\lstset{\n",
    "basicstyle=\\small\\ttfamily,\n",
    "columns=flexible,\n",
    "breaklines=true,\n",
    "mathescape\n",
    "}\n",
    "\n",
    "\\title{Implementing Reinforcement Learning Algorithms in Retail Supply Chains with OpenAI Gym Toolkit}\n",
    "\\author{Shaun D'Souza}\n",
    "\\date{}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\begin{abstract}\n",
    "From cutting costs to improving customer experience, forecasting is the crux of retail supply chain management (SCM) and the key to better supply chain performance. Several retailers are using AI/ML models to gather datasets and provide forecast guidance in applications such as Cognitive Demand Forecasting, Product End-of-Life, Forecasting, and Demand Integrated Product Flow. Early work in these areas looked at classical algorithms to improve on a gamut of challenges such as network flow and graphs. But the recent disruptions have made it critical for supply chains to have the resiliency to handle unexpected events.  The biggest challenge lies in matching supply with demand. \n",
    "\n",
    "Reinforcement Learning (RL) with its ability to train systems to respond to unforeseen environments, is being increasingly adopted in SCM to improve forecast accuracy, solve supply chain optimization challenges, and train systems to respond to unforeseen circumstances. Companies like UPS and Amazon have developed RL algorithms to define winning AI strategies and keep up with rising consumer delivery expectations. While there are many ways to build RL algorithms for supply chain use cases, the OpenAI Gym toolkit is becoming the preferred choice because of the robust framework for event-driven simulations. \n",
    "\n",
    "This white paper explores the application of RL in supply chain forecasting and describes how to build suitable RL models and algorithms by using the OpenAI Gym toolkit. \n",
    "\\end{abstract}\n",
    "\n",
    "%\\tableofcontents\n",
    "\n",
    "\\section{Why RL is Well Suited for Building AI Models for Supply Chains}\n",
    " \n",
    "The biggest challenge in building AI models for supply chains lies in matching supply with demand given the uncertain environment. Earlier attempts at building AI models for forecasting involved using time series modeling by using a combination of statistical and ML models to forecast sales based on the sales history and trends. But there were inherent limitations wherein such models could forecast numeric data but would be unable to determine policy dynamics. \n",
    "\n",
    "OpenAI Gym RL algorithms have several advantages that make them suitable for SCM such as the following: \\cite{openai, deepq}\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item \\textbf{Ability to optimize strategies and handle unexpected scenarios:} With their optimization procedures, RL algorithms can be used to find the best way to build predictive models that learn over time and earn maximum rewards, i.e. optimize strategies for best outcomes. In reinforcement learning, an agent takes an action in the given environment either in continuous or discrete manner to maximize its reward. Rewards are provided time stepwise and RL applications improve their performance by receiving rewards and punishments from the environment, and thereby determine the best action/strategy for handling situations.\n",
    "\\item \\textbf{Ability to cater to a diverse set of use cases:} AI models for SCM need to implement both discrete and continuous models because these cater to a diverse set of use cases in an ever-growing customer environment. By using a RL agent in conjunction with an event simulator available in the OpenAI Gym API, both discrete and continuous models can be built. With a robust framework for event-driven simulations delivered through an agent, actions, and a parameterized environment, OpenAI Gym with a Python-based agent work well for SCM AI models.\n",
    "\\item \\textbf{Ability to work with both structured and unstructured data:}  Python\\footnote{Note: The proliferation of off-the-shelf open source library APIs such as Google TensorFlow and Keras for high performance numerical computation facilitate the development of AI/ML based solutions using an open source development model. And, programming languages serve as an interface to the algorithms and lend a level of extensibility to users (AI experts, Researchers). Amongst them, Python with a large number of open-source libraries is the most popular.} can run sophisticated kernels owing to the availability of the NumPy and SciPy libraries. For example, Pandas, a Python library, provides support for importing and writing data sets in the widely used .xls and .csv formats. It also supports the JSON format making it possible to read a structured dataset with features and iterate through the rows in the data \\cite{sklearn}. However, unstructured data has to be parsed and then read using a visitor function. \n",
    "\\end{itemize}\n",
    "\n",
    "\\section{It is All About Real Data and Business Rules}\n",
    "\n",
    "AI/ML models learn outcomes from real data/parameters and business rules \\cite{rlrailway}. Let's understand how a parameterized RL model can be applied to supply chains with an example of promotional forecasting, i.e., predicting store sales for a given calendar day based on historical sales data. Typically such promotions are governed by retailer goals and constraints/business rules.\n",
    " \n",
    "For example, some of the available data points/parameters in a typical forecasting model are summarized in Listings ~\\ref{promotionplan} -~\\ref{rxtransactions}. The data points provided are typically evaluated in conjunction with business rules such as seasonality, promotional events, day of the week, and state or school holidays, and they form the basis of the parameterized environment. \n",
    "\n",
    "\\begin{lstlisting}[caption=Promotion Plan, label=promotionplan]\n",
    "Promo Code      Promo Type      Event ID        Promo Start Date        Promo End Date  Promo Target/Projection Amount Store IAd ID/Block ID   Product ID      Offer Qty Offer Price   Planogram Change Indicator      Special Package Indicator       Ad Location Indicator  Coupon Indicator\n",
    "\\end{lstlisting}\n",
    "\n",
    "\\begin{lstlisting}[caption=Online Transactions, label=onlinetransactions]\n",
    "Product ID      Date    EoD Sales Qty   EoD Return Qty  Zip Code        City    State Geo Area Code\n",
    "\\end{lstlisting}\n",
    "\n",
    "\\begin{lstlisting}[caption=Rx Transactions, label=rxtransactions]\n",
    "Store ID        Product ID      Date EoD Sales Qty      Qty UoM\n",
    "\\end{lstlisting}\n",
    "\n",
    "Figure ~\\ref{fig:percentile} shows the sales for five largest categorical sales bins captured by day of the week. Generally there are several thousand data points that need to be analyzed by AI models. Today's AI models are unable to forecast using a lakh or million records and therefore it is a common practice to create subsets of data for modeling. These subsets are referred to as categorical bins; it is like a slice of data representative of the original dataset where the discrete inputs are masked.  And, these categorical bins facilitate the creation of macro models. In this example, we have used 5 categorical bins based on unit sales.\n",
    "\n",
    "\\begin{figure}[!h]\n",
    "    \\centering\n",
    "   \\includegraphics[width=\\columnwidth]{percentile}\n",
    "   \\caption{Percentile categorical and total sales by day of the week}\n",
    "   \\label{fig:percentile}\n",
    "\\end{figure}\n",
    "\n",
    "As shown in Figure ~\\ref{fig:percentile}, the median sales were higher on the day of a promotional event. It is imperative for retailers to forecast sales accurately to avoid a shortfall in sales on the day of event. This can be complex as store sales vary based on multiple factors including promotions, competition, school and state holidays, seasonality, locality, etc.\n",
    "\n",
    "\\section{Building an RL Model for Forecasting: An Example}\n",
    "\n",
    "Let's see how a parameterized RL model can be used for accurate demand forecasting.\n",
    "\n",
    "\\begin{enumerate}\n",
    "\n",
    "\\item \\textbf{Determine the AI Model: Discrete Vs Continuous:} Considering that the dimensionality of the data sets will grow exponentially, i.e. all permutations and combinations of the goals and the constraints have to be factored, a discrete AI model is best suited. For example, the goals here are the net unit sales for the promotion and the constraints could be any of the parameterized factors such as promotions, seasonality, competition,\n",
    "and locality.\n",
    " \n",
    "An N-dimensional network is defined using the discrete.DiscreteEnv class. The environment is initialized using a static set of maps that hold the transition probabilities and reward functions. For continuous implementations such as the Cart Pole, use the gym.Env \\cite{cartpole}. The code snippet (See Listing ~\\ref{frozenlake}) shows how to initialize the Frozen-Lake environment for a run of 100 timesteps. The render function is used in an episode (sequence of events in RL) to visualize the observation space. \n",
    "\n",
    "\\begin{lstlisting}[caption=Frozen Lake, label=frozenlake]\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "env.render()\n",
    "env.step(env.action_space.sample()) # take a random action\n",
    "env.close()\n",
    "\\end{lstlisting}\n",
    "\n",
    "\\item \\textbf{Define the RL Agent:} Next, we define a RL agent that learns the Frozen lake environment based on the observation and reward from episodic events \\cite{rlbook}. An episode is a sequence of steps. Actions and rewards are defined for each episode. RL scenarios for an agent in deterministic environment can be formulated as dynamic programming problem. This can be done using the Temporal Difference Learning and Q-Learning functions. RL algorithms consists of online and offline approaches. Online RL algorithms, such as Q-learning explore a domain while learning a policy. Offline approaches use a distinct sample collection phase and learning phases. Each has advantages and disadvantages. Online approaches are computationally efficient and adapt quickly to new observations. More complex real-world problems might utilize an offline algorithm wherein a subset of the data is used to model the actual environment.\n",
    "\n",
    "In this example, the agent implements a Q-learning based RL step (See Listing ~\\ref{qlearning}) to determine the optimal reward function in the customer sales environment. The goal of the agent is to maximize the total reward for any exploratory path in the episode, i.e.  the agent has to perform a series of steps in a systematic manner so that it can learn the ideal solution based on the reward values. \n",
    "\n",
    "\\begin{lstlisting}[caption=Q-Learning, label=qlearning]\n",
    "Initialize Q(s,a), $\\forall$ s $\\epsilon$ S, a $\\epsilon$ A(s), arbitrarily, and Q(terminal-state,.) = 0\n",
    "   Repeat (for each episode):\n",
    "   Initialize S\n",
    "   Repeat (for each step of episode):\n",
    "        Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy)\n",
    "   Take action A, observe R, S'\n",
    "   Q(S,A) $\\leftarrow$ Q(S,A) + $\\alpha$[R + $\\gamma$ max Q(S',a) - Q(S,A)]\n",
    "   S $\\leftarrow$ S'\n",
    "   until S is terminal\n",
    "\\end{lstlisting}\n",
    "\n",
    "An 'act' function is defined on the agent to determine the appropriate action, followed by a 'step' function to evaluate the outcome of the action in the current environment (see Figure ~\\ref{fig:rlagent}). The agent perceives the environment and performs actions and determines the optimal reward in the customer sales environment. \n",
    "\n",
    "\\begin{figure}[!h]\n",
    "    \\centering\n",
    "   \\includegraphics[width=0.5\\columnwidth]{rlagent.pdf}\n",
    "   \\caption{RL agent in an environment}\n",
    "   \\label{fig:rlagent}\n",
    "\\end{figure}\n",
    "\n",
    "For large state space exploration as in AlphaGo and related DeepMind projects, the agents make use of Deep Learning (DL) techniques \\cite{deepgame}.\n",
    "\n",
    "\\item \\textbf{Train the RL model:} The model is trained based on key parameters in the dataset such as sales, promo, state holiday, school holiday and day of week. It monitors the sales and adjusts the inventory by using the increase and lower inventory functions. \n",
    "\n",
    "If the model is unable to find a promotional channel, it implements a realign operation. This ensures that the episode aligns with the available promotional events as defined in the forecasting dataset; in other words, it implements the business rule to check if there is an active promotion on the channel. This is implemented by using the OpenAI Gym transitions probability matrix discrete.DiscreteEnv.P. The probability matrix P contains the transition mapping for a state and action taken, as well as the possible next states. These are defined using the template in Listing ~\\ref{template}.\n",
    "\n",
    "\\begin{lstlisting}[caption=Transition probabilities matrix template, label=template]\n",
    "{state : { actions : [ (probability, next states, reward, done) ] } }\n",
    "\\end{lstlisting}\n",
    "\n",
    "\\begin{lstlisting}[caption=Transition probabilities, label=probabilities]\n",
    "  35: { 0: [ (0.14285714285714285, 30, -1, False),\n",
    "             (0.14285714285714285, 31, -1, False),\n",
    "             (0.14285714285714285, 32, -1, False),\n",
    "             (0.14285714285714285, 33, -1, False),\n",
    "             (0.14285714285714285, 34, -1, False),\n",
    "             (0.14285714285714285, 35, -1, False),\n",
    "             (0.14285714285714285, 38, -1, False)],\n",
    "        1: [(1.0, 25, -1, False)],\n",
    "        2: [(1.0, 45, -1, False)],\n",
    "        3: [(1.0, 35, -10, False)]},\n",
    "  36: { 0: [ (0.14285714285714285, 30, -1, False),\n",
    "             (0.14285714285714285, 31, -1, False),\n",
    "             (0.14285714285714285, 32, -1, False),\n",
    "             (0.14285714285714285, 33, -1, False),\n",
    "             (0.14285714285714285, 34, -1, False),\n",
    "             (0.14285714285714285, 35, -1, False),\n",
    "             (0.14285714285714285, 38, -1, False)],\n",
    "        1: [(1.0, 26, -1, False)],\n",
    "        2: [(1.0, 46, -1, False)],\n",
    "        3: [(1.0, 36, -10, False)]},\n",
    "\\end{lstlisting}\n",
    "\n",
    "\\end{enumerate}\n",
    "\n",
    "In forecasting promotions, the episode either ends with a promotion or no promotion on a specific day of the week. The next-state feature corresponding to any non-terminal promotional paths (unexpected event in the context of forecasting) are chained to the closest available matches using the input dataset (see Listing ~\\ref{probabilities}). \n",
    "\n",
    "For example, if the retailer does not run a promotion during a particular week, the model moves to forecast the inventory required for the next event (promotional event or seasonal event).  The model determines the closest available match by using a subset of the matching records to determine the optimal reward at training time for the realign action at each step, i.e. it can either increase, lower, realign, or forecast a promotion. This ensures that the model continuously forecasts inventory irrespective of whether there is an event or not, providing optimal forecasts based on customer data.\n",
    "\n",
    "\\begin{figure}\n",
    "  \\centering\n",
    "  \\includegraphics[width=.8\\columnwidth]{meanreward}\n",
    "  \\caption{Mean cumulative reward}\n",
    "  \\label{fig:meanreward}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}\n",
    "  \\centering\n",
    "  \\includegraphics[width=.8\\columnwidth]{episodicreward}\n",
    "  \\caption{Episodic reward}\n",
    "  \\label{fig:episodicreward}\n",
    "\\end{figure}\n",
    "\n",
    "Plot in Figure ~\\ref{fig:meanreward} shows the mean cumulative reward as a function of the steps for an independent set of episodes. As we see the reward value\n",
    "gradually saturates in the number of training steps. Figure ~\\ref{fig:episodicreward} shows a sample episode for a trained RL model.\n",
    "\n",
    "\\section{Key Findings Indicate Accuracy of RL for Supply Chain Modeling}\n",
    "\n",
    "\\begin{figure}[!h]\n",
    "\\begin{minipage}{.5\\textwidth}\n",
    "    \\centering\n",
    "   \\includegraphics[width=.9\\textwidth]{increasesales.pdf}\n",
    "   \\caption{Increase Sales Promotion episode}\n",
    "   \\label{fig:increasesales}\n",
    "%\\end{figure}\n",
    "\\end{minipage}\n",
    "\\begin{minipage}{0.5\\textwidth}\n",
    "%\\begin{figure}[!h]\n",
    "    \\centering\n",
    "   \\includegraphics[width=.9\\textwidth]{lowersales.pdf}\n",
    "   \\caption{Lower Sales Promotion episode}\n",
    "   \\label{fig:lowersales}\n",
    "\\end{minipage}\n",
    "\\end{figure}\n",
    "\n",
    "Figures ~\\ref{fig:increasesales} and ~\\ref{fig:lowersales} show sample episodes. The green circles show the sales captured by the day of the week. We see a steady increase in sales on day-to-day basis leading up to a promotional event. Each set of rows in the figure depict the days of the week corresponding to the episode. Similarly, in Figure ~\\ref{fig:lowersales}, we see a lower sales event leading up to a promotion. Realign operations ensure that promotion events for seasonality are forecasted past the attributes in the historical data. The findings indicate that RL models can accurately model forecasting and promotional events corresponding to inventory sales. They capture nuances in the dataset, such as variations in sales from day of week or month of year. \n",
    "\n",
    "\\section{Conclusion}\n",
    "We evaluated the suitability of  RL models built with the OpenAI Gym framework for retail SCM. The example of the forecasting model captures forecasting data into a learning algorithm that provides guidance to retailers on the inventory to be stocked in the DC. This model can be tailored to meet the constraints of customer data and advanced use cases in SCM. \n",
    "\n",
    "In a discrete RL environment such as the one configured in the example, the model can provide a guide of the possible outcomes in a producer-consumer model using a many-to-one scenario where you could have 4 producers mapped to a single consumer. Because OpenAI  Gym offers a high degree of programmability, enabling both elementary linear maps and higher-dimensional policy functionalities, both simple and sophisticated environments can be modeled to cater to supply chain scenarios. \n",
    "\n",
    "\\begin{thebibliography}{9}\n",
    "   \\bibitem{openai}\n",
    "        arXiv, \n",
    "        'OpenAI Gym' (5 June 2016),\n",
    "        accessed September 2020, \n",
    "        https://arxiv.org/abs/1606.01540\n",
    "\n",
    "   \\bibitem{deepq}\n",
    "        arXiv, \n",
    "        'A Deep Q-Network for the Beer Game: A Deep Reinforcement Learning Algorithm to Solve Inventory Optimization Problems' (2017), \n",
    "        accessed September 2020, \n",
    "        https://arxiv.org/abs/1708.05924\n",
    "\n",
    "   \\bibitem{sklearn}\n",
    "        The Journal of Machine Learning Research, \n",
    "        'Scikit-learn: Machine Learning in Python' (2011), \n",
    "        accessed September 2020, \n",
    "        https://jmlr.org/papers/v12/pedregosa11a.html\n",
    "\n",
    "   \\bibitem{rlrailway}\n",
    "        IEEE Transactions on Intelligent Transportation Systems, \n",
    "        'A scalable reinforcement learning algorithm for scheduling railway lines' (2018), \n",
    "        accessed October 2020, \n",
    "        https://ieeexplore.ieee.org/document/8357909\n",
    "\n",
    "   \\bibitem{cartpole}\n",
    "        'Classic cart-pole system', \n",
    "        https://perma.cc/C9ZM-652R\n",
    "\n",
    "   \\bibitem{rlbook}\n",
    "        MIT Press, \n",
    "        'Reinforcement Learning: An Introduction' (November 2018), \n",
    "        accessed October 2020, \n",
    "        https://mitpress.mit.edu/books/reinforcement-learning-second-edition\n",
    "\n",
    "   \\bibitem{deepgame}\n",
    "        Advances in Neural Information Processing Systems, \n",
    "        'Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning' (2014), \n",
    "        accessed October 2020, \n",
    "        https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning\n",
    "\n",
    "\\end{thebibliography}\n",
    "\n",
    "\\section{About the Author}\n",
    "\n",
    "\\textbf{Shaun D'Souza}\n",
    "\n",
    "Data Scientist, TCS\n",
    "\n",
    "Shaun has over 12 years of experience in AI, ML, Software Engineering, R\\&D, and Business. He has a Bachelor's degree from Cornell University with a Double Major in Computer Science, Electrical and Computer Engineering, Cum Laude Honors, and a Masters in Electrical Engineering from the University of Michigan. Shaun has worked in Software/Business researching machine learning, compilers, algorithms, and systems. He has published several papers and has been granted a patent.\n",
    "\n",
    "\\end{document}'''\n",
    "matches = re.match(r'^\\w.*\\.', paragraph)\n",
    "\n",
    "sentences = [sentence.strip(\" \") for sentence in re.split(r'\\.|\\?', paragraph)]\n",
    "\n",
    "sample_category = random.choices(list(configuration.keys()), weights = list(configuration.values()))[0]\n",
    "sample_prompt = random.choice(prompts[sample_category])\n",
    "sample_sentence = random.choice(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply\n",
      "ieee\n",
      "What's next?\n"
     ]
    }
   ],
   "source": [
    "print(sample_category)\n",
    "print(sample_sentence)\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r'^\\w.*\\.', paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
